{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 丢弃法在训练模型时起到正则化的作用，并可以用来应对过拟合。\n",
    "# 在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。\n",
    "\n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 倒置丢弃法（inverted dropout）\n",
    "\n",
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    # 下面的比较会采用广播机制，返回一个真值向量\n",
    "    # 这是获取特定概率分布数据的技巧\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  0.   0.   0.   0.   0.   0.   0.   0.]\n",
       " [ 16.   0.   0.  22.  24.  26.   0.  30.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "\n",
    "X = nd.arange(16).reshape((2, 8))\n",
    "#dropout(X, 0)\n",
    "dropout(X, 0.5)\n",
    "#dropout(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型参数\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "\n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    # 通过reshape函数将每张原始图像改成长度为num_inputs的向量\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X, W1) + b1).relu()\n",
    "    # 只在训练的时候使用丢弃法\n",
    "    if autograd.is_training():\n",
    "        H1 = dropout(H1, drop_prob1)\n",
    "    H2 = (nd.dot(H1, W2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1926, train acc 0.535, test acc 0.771\n",
      "epoch 2, loss 0.6001, train acc 0.775, test acc 0.833\n",
      "epoch 3, loss 0.5047, train acc 0.816, test acc 0.844\n",
      "epoch 4, loss 0.4544, train acc 0.835, test acc 0.857\n",
      "epoch 5, loss 0.4242, train acc 0.845, test acc 0.863\n"
     ]
    }
   ],
   "source": [
    "# 训练及测试\n",
    "\n",
    "num_epochs, lr, batch_size = 5, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简洁实现\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(256),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.4464, train acc 0.469, test acc 0.761\n",
      "epoch 2, loss 0.6815, train acc 0.747, test acc 0.821\n",
      "epoch 3, loss 0.5838, train acc 0.785, test acc 0.798\n",
      "epoch 4, loss 0.5169, train acc 0.812, test acc 0.844\n",
      "epoch 5, loss 0.5777, train acc 0.794, test acc 0.837\n"
     ]
    }
   ],
   "source": [
    "# 训练及测试\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习:\n",
    "\n",
    "如果把本节中的两个丢弃概率超参数对调，会有什么结果？\n",
    "\n",
    "增大迭代周期数，比较使用丢弃法与不使用丢弃法的结果。\n",
    "\n",
    "如果将模型改得更加复杂，如增加隐藏层单元，使用丢弃法应对过拟合的效果是否更加明显？\n",
    "\n",
    "以本节中的模型为例，比较使用丢弃法与权重衰减的效果。如果同时使用丢弃法和权重衰减，效果会如何？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
